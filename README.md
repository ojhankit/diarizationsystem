# ğŸ™ï¸ Speaker Diarization Pipeline

An end-to-end **speaker diarization system** built with [Pyannote](https://github.com/pyannote/pyannote-audio), **PyTorch**, and **scikit-learn**.  
This repo takes raw audio files â†’ detects speech â†’ extracts embeddings â†’ clusters â†’ outputs RTTM files.  

> **Recent Improvements ğŸš€**  
> - âš¡ GPU acceleration for fast embedding extraction  
> - ğŸ“ˆ Eigengap heuristic for smarter speaker estimation  
> - ğŸ§¹ Affinity thresholding to reduce noise in similarity matrices  
> - ğŸ”’ Robust handling of short / invalid segments  
> - ğŸ“ Automatic RTTM file renaming + filename consistency  
> - ğŸ”„ Re-clustering refinement to reduce speaker confusion  

---

## âœ¨ Features

- ğŸ”Š **VAD (Voice Activity Detection)** â†’ detects speech regions from audio  
- ğŸ§© **Speaker Embedding Extraction** with `pyannote/embedding`  
  - Runs on **GPU** (CUDA) for maximum speed  
  - Handles NaN/Inf and zero vectors safely  
  - Skips very short segments (<0.5s)  
- ğŸ“Š **First-Pass Clustering** (Spectral Clustering)  
  - **Eigengap heuristic** to dynamically estimate speaker count  
  - Affinity thresholding to ignore weak connections  
- ğŸ”„ **Re-clustering Refinement** (Hierarchical Agglomerative Clustering)  
  - Iterative merging of small/close clusters  
  - Reduces **confusion errors** in DER/JER  
- ğŸ“ **RTTM Handling**  
  - Automatically names RTTM outputs as `<audio_name>_diarization.rttm`  
  - Updates `SPEAKER` lines to include correct file IDs  
- ğŸ–¥ï¸ **Detailed Logging** for full transparency at each stage  

---



